{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkConf\n",
    "\n",
    "import pyspark.sql as pysql\n",
    "\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml as pyml\n",
    "import pyspark.sql.functions as pysqlfun\n",
    "import pyspark.sql.types as pysqltypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "Para la materia de Datos Masivos de la Maestría de Ciencias de Datos realicé este notebook de Jupyter el cual predice si tu vuelo será cancelado o no.\n",
    "\n",
    "Los datos se obtuvieron en <https://www.kaggle.com/datasets/robikscube/flight-delay-dataset-20182022> la cual consiste en vuelos nacionales en Estados Unidos del 2018 al 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"Tarea\") \\\n",
    "    .setMaster(\"local[8]\") \\\n",
    "    .set(\"spark.executor.cores\",\"2\") \\\n",
    "    .set(\"spark.executor.memory\", \"4g\") \\\n",
    "    .set(\"spark.driver.memory\",\"4g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark : SparkSession  = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = float(5000.0) #mayor a 0 (en millas)\n",
    "year = 2024 #año\n",
    "month = 11 #mes (1 a 12)\n",
    "day_of_month = 28 #dia del mes del vuelo\n",
    "day_of_week = 5 #Dia de la semana del vuelo (1 domingo a 7 sabado)\n",
    "airline = \"United Air Lines Inc.\" #aerolinea, revisar valores en airlines.csv\n",
    "tail_number = \"N480HA\" #Código de identificación del avión, revisar valores en df_tail_number.csv\n",
    "origin = \"LAX\" #Código del aeropuerto de origen, revisar valores en airports_origin.csv y/o df_origin.csv\n",
    "dest = \"JFK\" #Código del aeropuerto de destino, revisar valores en airports_dest.csv y/o df_dest.csv\n",
    "origin_state_name = \"California\" #Nombre del estado de origen del vuelo, revisar valores en df_origin_state.csv\n",
    "dest_state_name = \"New York\" #Nombre del estado de destino del vuelo, revisar valores en df_dest_state.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = spark.createDataFrame(\n",
    "    [pysql.Row(Distance = distance, Year = year, Month = month,\n",
    "               DayofMonth = day_of_month, DayOfWeek = day_of_week,\n",
    "               Airline = airline, Tail_Number = tail_number,\n",
    "               Origin = origin, Dest = dest,\n",
    "               OriginStateName = origin_state_name,\n",
    "               DestStateName = dest_state_name)],\n",
    "    schema = pysqltypes.StructType([\n",
    "        pysqltypes.StructField(\"Distance\", pysqltypes.FloatType(), True),\n",
    "        pysqltypes.StructField(\"Year\", pysqltypes.IntegerType(), True),\n",
    "        pysqltypes.StructField(\"Month\", pysqltypes.IntegerType(), True),\n",
    "        pysqltypes.StructField(\"DayofMonth\", pysqltypes.IntegerType(), True),\n",
    "        pysqltypes.StructField(\"DayOfWeek\", pysqltypes.IntegerType(), True),\n",
    "        pysqltypes.StructField(\"Airline\", pysqltypes.StringType(), True),\n",
    "        pysqltypes.StructField(\"Tail_Number\", pysqltypes.StringType(), True),\n",
    "        pysqltypes.StructField(\"Origin\", pysqltypes.StringType(), True),\n",
    "        pysqltypes.StructField(\"Dest\", pysqltypes.StringType(), True),\n",
    "        pysqltypes.StructField(\"OriginStateName\", pysqltypes.StringType(), True),\n",
    "        pysqltypes.StructField(\"DestStateName\", pysqltypes.StringType(), True)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tail_number_column(df):\n",
    "    columna = \"Tail_Number_Grupo\"\n",
    "    direccion = pathlib.Path.cwd()\n",
    "    archivo = \"df_tail_number.csv\"\n",
    "    \n",
    "    path_df = pathlib.Path.joinpath(direccion, \"Tareas\",archivo).as_posix()\n",
    "    \n",
    "    df_paso = spark.read.csv(path=path_df, header=True)\n",
    "    \n",
    "    df = df.join(other = df_paso, on = \"Tail_Number\", how = \"left\")\n",
    "    \n",
    "    df = df.withColumn(columna, pysqlfun.when(\n",
    "        df[columna].isNull() | (df[columna] == \"None\") \n",
    "            | (df[columna] == \"Otros\"),0).otherwise(df[columna])\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def origin_column(df):\n",
    "    columna = \"Origin_Grupo\"\n",
    "    direccion = pathlib.Path.cwd()\n",
    "    archivo = \"df_origin.csv\"\n",
    "    \n",
    "    path_df = pathlib.Path.joinpath(direccion, \"Tareas\", archivo).as_posix()\n",
    "    \n",
    "    df_paso = spark.read.csv(path=path_df, header=True)\n",
    "    \n",
    "    df = df.join(other = df_paso, on = \"Origin\", how = \"left\")\n",
    "    \n",
    "    df = df.withColumn(columna, pysqlfun.when(\n",
    "        df[columna].isNull() | (df[columna] == \"None\") \n",
    "            | (df[columna] == \"Otros\"),0).otherwise(df[columna])\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dest_column(df):\n",
    "    columna = \"Dest_Grupo\"\n",
    "    direccion = pathlib.Path.cwd()\n",
    "    archivo = \"df_dest.csv\"\n",
    "    \n",
    "    path_df = pathlib.Path.joinpath(direccion, \"Tareas\", archivo).as_posix()\n",
    "    \n",
    "    df_paso = spark.read.csv(path=path_df, header=True)\n",
    "    \n",
    "    df = df.join(other = df_paso, on = \"Dest\", how = \"left\")\n",
    "    \n",
    "    df = df.withColumn(columna, pysqlfun.when(\n",
    "        df[columna].isNull() | (df[columna] == \"None\") \n",
    "            | (df[columna] == \"Otros\"),0).otherwise(df[columna])\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def origin_state_column(df):\n",
    "    columna = \"OriginStateName_Grupo\"\n",
    "    direccion = pathlib.Path.cwd()\n",
    "    archivo = \"df_origin_state.csv\"\n",
    "    \n",
    "    path_df = pathlib.Path.joinpath(direccion, \"Tareas\", archivo).as_posix()\n",
    "    \n",
    "    df_paso = spark.read.csv(path=path_df, header=True)\n",
    "    \n",
    "    df = df.join(other = df_paso, on = \"OriginStateName\", how = \"left\")\n",
    "    \n",
    "    df = df.withColumn(columna, pysqlfun.when(\n",
    "        df[columna].isNull() | (df[columna] == \"None\") \n",
    "            | (df[columna] == \"Otros\"),0).otherwise(df[columna])\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dest_state_column(df):\n",
    "    columna = \"DestStateName_Grupo\"\n",
    "    direccion = pathlib.Path.cwd()\n",
    "    archivo = \"df_dest_state.csv\"\n",
    "    \n",
    "    path_df = pathlib.Path.joinpath(direccion, \"Tareas\", archivo).as_posix()\n",
    "    \n",
    "    df_paso = spark.read.csv(path=path_df, header=True)\n",
    "    \n",
    "    df = df.join(other = df_paso, on = \"DestStateName\", how = \"left\")\n",
    "    \n",
    "    df = df.withColumn(columna, pysqlfun.when(\n",
    "        df[columna].isNull() | (df[columna] == \"None\") \n",
    "            | (df[columna] == \"Otros\"),0).otherwise(df[columna])\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_input\n",
    "for columna in [\"Airline\", \"Tail_Number\", \"Origin\", \"Dest\", \"OriginStateName\", \"DestStateName\"]:\n",
    "    df = df.withColumn(columna, pysqlfun.when(\n",
    "        df[columna].isNull() | (df[columna] == \"None\"),\"Otros\").otherwise(df[columna]))\n",
    "    \n",
    "df = tail_number_column(df)\n",
    "df = origin_column(df)\n",
    "df = dest_column(df)\n",
    "df = origin_state_column(df)\n",
    "df = dest_state_column(df)\n",
    "\n",
    "df_seleccion = df.select([\"Distance\",\n",
    "    \"Year\",\"Month\",\"DayOfWeek\",\"DayofMonth\",\"Airline\",\n",
    "    \"Tail_Number_Grupo\",\"origin_Grupo\",\"Dest_Grupo\",\n",
    "    \"OriginStateName_Grupo\",\"DestStateName_Grupo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "direccion = pathlib.Path.cwd()\n",
    "    \n",
    "path = pathlib.Path.joinpath(direccion, \"Tareas\", \"indexer\").as_posix()\n",
    "\n",
    "indexer = pyml.feature.StringIndexerModel.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexado = indexer.transform(df_seleccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = pyml.feature.VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"Distance\",\n",
    "        \"Year\",\n",
    "        \"Month\",\n",
    "        \"DayofMonth\",\n",
    "        \"DayOfWeek\",\n",
    "        \"Airline_indexado\",\n",
    "        \"Tail_Number_indexado\",\n",
    "        \"Origin_indexado\",\n",
    "        \"Dest_indexado\",\n",
    "        \"OriginStateName_indexado\",\n",
    "        \"DestStateName_indexado\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_with_features = assembler.transform(df_indexado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "direccion = pathlib.Path.cwd()\n",
    "    \n",
    "path = pathlib.Path.joinpath(direccion, \"Tareas\", \"RandomForest_model\").as_posix()\n",
    "\n",
    "modelo = pyml.classification.RandomForestClassificationModel.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediccion\n",
    "prediccion = modelo.transform(df_with_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result = prediccion.select([\"probability\",\"prediction\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = df_result[\"prediction\"].to_list()[0]\n",
    "prediccion = int(prediccion)\n",
    "\n",
    "probabilidad = df_result[\"probability\"].to_list()\n",
    "prediccion = probabilidad[0][prediccion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proyección estado de tu vuelo: Normal, con una probabilidad de 51.64%\n"
     ]
    }
   ],
   "source": [
    "resultado = \"Cancelado\" if prediccion == 1 else \"Normal\"\n",
    "\n",
    "print(f\"Proyección estado de tu vuelo: {resultado}, con una probabilidad de {100*prediccion:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
